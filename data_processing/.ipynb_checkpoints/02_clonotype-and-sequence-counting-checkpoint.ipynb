{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clonotype and sequence counting\n",
    "\n",
    "Starting with deduplicated clonotypes or sequences, count the occurrence of repeatedly observed clonotypes/sequences (seen in multiple biological replicates from the same subject) or shared clonotypes/sequences (seen in multiple subjects).\n",
    "\n",
    "The [`abutils`](https://www.github.com/briney/abutils) Python package is required, and can be installed by running `pip install abutils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from abutils.utils.jobs import monitor_mp_jobs\n",
    "from abutils.utils.pipeline import list_files, make_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files and directories\n",
    "\n",
    "There are two input data directories each for clonotypes and sequences -- one containing deduplicated single-subject pools (to quantify repeatedly observed clonotypes/sequences) and another containing deduplicated cross-subject pools (to quantify shared clonotypes/sequences). The input data used by the following code is too large to be included in this repository. Input datasets can be generated using the code in [**this Jupyter notebook**](LINK). Alternatively, data can be downloaded from the following links:\n",
    "  * single-subject clonotype data can be downloaded [**here**](LINK)\n",
    "  * cross-subject clonotype data can be downloaded [**here**](LINK)\n",
    "  * single-subject sequence data can be downloaded [**here**](LINK)\n",
    "  * cross-subject sequence data can be downloaded [**here**](LINK)\n",
    "\n",
    "If generating the input data using the code in the referenced Jupyter notebook, the data should be deposited into the appropriate directory. If downloading the data, either decompress the downloaded data file in the appropriate directory or modify the `single_subject_dir` and/or `cross_subject_dir` variables as needed.\n",
    "\n",
    "***NOTE:*** *The uncompressed cross-subject input data is quite large (>2TB for clonotypes and >20TB for sequences). Ensure that you have sufficient storage before downloading and decompressing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "single_subject_clonotype_dir = './data/dedup_subject_clonotype_pools/'\n",
    "cross_subject_clonotype_dir = './data/dedup_cross-subject_clonotype_pools/'\n",
    "clonotype_output_dir = './data/user-calculated_cross-subject_clonotype_duplicate-counts/'\n",
    "single_subject_sequence_dir = './data/dedup_subject_sequence_pools/'\n",
    "cross_subject_sequence_dir = './data/dedup_cross-subject_sequence_pools/'\n",
    "sequence_output_dir = './data/user-calculated_cross-subject_sequence_duplicate-counts/'\n",
    "make_dir(clonotype_output_dir)\n",
    "make_dir(sequence_output_dir)\n",
    "\n",
    "# files\n",
    "clonotype_files = [f for f in list_files(single_subject_clonotype_dir) if 'pool_vj-aa_with-counts.txt' in f]\n",
    "clonotype_files += [f for f in list_files(cross_subject_clonotype_dir) if 'pool_vj-aa_with-counts.txt' in f]\n",
    "sequence_files = [f for f in list_files(single_subject_sequence_dir) if 'pool_nt-seq_with-counts.txt' in f]\n",
    "sequence_files += [f for f in list_files(cross_subject_sequence_dir) if 'pool_nt-seq_with-counts.txt' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clonotypes\n",
    "clonotype_files_by_subject_count = {i: [] for i in range(1, 11)}\n",
    "for f in clonotype_files:\n",
    "    num = len(os.path.basename(f).split('_')[0].split('-'))\n",
    "    clonotype_files_by_subject_count[num].append(f)\n",
    "\n",
    "# sequences\n",
    "sequence_files_by_subject_count = {i: [] for i in range(1, 11)}\n",
    "for f in clonotype_files:\n",
    "    num = len(os.path.basename(f).split('_')[0].split('-'))\n",
    "    sequence_files_by_subject_count[num].append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate counting\n",
    "\n",
    "Now we'd like to count the duplicate (repeatedly observed or shared) clonotypes for every groupwise combination of our 10 subjects. Each group can contain one or more subjects, meaning the total number of possible groupwise combinations is quite large. We'll use the `multiprocessing` package to parallelize the process which should speed things up substantially, although even with parallelization, this will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicates(input_file, output_dir):\n",
    "    counts = {str(i): 0 for i in range(1, 11)}\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            c = line.strip().split()[0]\n",
    "            counts[c] += 1\n",
    "    subject_prefix = os.path.basename(input_file).split('_')[0]\n",
    "    output_file = os.path.join(output_dir, '{}_occurrence-counts.txt'.format(subject_prefix))\n",
    "    with open(output_file, 'w') as f:\n",
    "        data = ['{}\\t{}'.format(k, v) for k, v in sorted(counts.items(), key=lambda x: int(x[0]))]\n",
    "        data_string = '\\n'.join(data)\n",
    "        f.write(data_string) \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clonotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.Pool(maxtasksperchild=1)\n",
    "clonotype_counts = {}\n",
    "\n",
    "for num in clonotype_files_by_subject_count.keys():\n",
    "    async_results = []\n",
    "    print('subject count:', num)\n",
    "    sys.stdout.flush()\n",
    "    for ifile in files_by_subject_count[num]:\n",
    "        async_results.append(p.apply_async(count_duplicates, args=(ifile, clonotype_output_dir)))\n",
    "    monitor_mp_jobs(async_results)\n",
    "    clonotype_counts[num] = [ar.get() for ar in async_results]\n",
    "    print('\\n')\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.Pool(maxtasksperchild=1)\n",
    "sequence_counts = {}\n",
    "\n",
    "for num in sequence_files_by_subject_count.keys():\n",
    "    async_results = []\n",
    "    print('subject count:', num)\n",
    "    sys.stdout.flush()\n",
    "    for ifile in files_by_subject_count[num]:\n",
    "        async_results.append(p.apply_async(count_duplicates, args=(ifile, sequence_output_dir)))\n",
    "    monitor_mp_jobs(async_results)\n",
    "    sequence_counts[num] = [ar.get() for ar in async_results]\n",
    "    print('\\n')\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
